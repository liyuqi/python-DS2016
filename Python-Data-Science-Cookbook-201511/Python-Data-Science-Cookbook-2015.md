Ref:
https://www.packtpub.com/big-data-and-business-intelligence/python-data-science-cookbook
http://www.allitebooks.com/python-data-science-cookbook/
----------------------------
Book covers: p36
Chapter 1, Python for Data Science, introduces Python’s built-in data structures and
functions, which are very handy for data science programming.
Chapter 2, Python Environments, introduces Python’s scientific programming and plotting
libraries, including NumPy, matplotlib, and scikit-learn.
Chapter 3, Data Analysis – Explore and wrangle, covers data preprocessing and
transformation routines to perform exploratory data analysis tasks in order to efficiently
build data science algorithms.
Chapter 4, Data Analysis – Deep Dive, introduces the concept of dimensionality reduction
in order to tackle the curse of dimensionality issues in data science. Starting with simple
methods and moving on to the advanced state-of-the-art dimensionality reduction
techniques are discussed in detail.
Chapter 5, Data Mining – Needle in a haystack Name, discusses unsupervised data mining
techniques, starting with elaborate discussions on distance methods and kernel methods
and following it up with clustering and outlier detection techniques.
Chapter 6, Machine Learning 1, covers supervised data mining techniques, including
nearest neighbors, Naïve Bayes, and classification trees. In the beginning, we will lay a
heavy emphasis on data preparation for supervised learning.
Chapter 7, Machine Learning 2, introduces regression problems and follows it up with
topics on regularization including LASSO and ridge. Finally, we will discuss crossvalidation
techniques as a way to choose hyperparameters for these methods.
Chapter 8, Ensemble Methods, introduces various ensemble techniques including bagging,
boosting, and gradient boosting This chapter shows you how to make a powerful state-ofthe-art
method in data science where, instead of building a single model for a given
problem, an ensemble or a bag of models are built.
Chapter 9, Growing Trees, introduces some more bagging methods based on tree-based
algorithms. Due to their robustness to noise and universal applicability to a variety of
problems, they are very popular among the data science community.
Chapter 10, Large scale machine learning – Online Learning, covers large scale machine
learning and algorithms suited to tackle such large scale problems. This includes
algorithms that work with streaming data and data that cannot be fitted into memory
completely.

----------------------------
Table Contents:
1: PYTHON FOR DATA SCIENCE
    Introduction
    Using dictionary objects
    Working with a dictionary of dictionaries
    Working with tuples
    Using sets
    Writing a list
    Creating a list from another list - list comprehension
    Using iterators
    Generating an iterator and a generator
    Using iterables
    Passing a function as a variable
    Embedding functions in another function
    Passing a function as a parameter
    Returning a function
    Altering the function behavior with decorators
    Creating anonymous functions with lambda
    Using the map function
    Working with filters
    Using zip and izip
    Processing arrays from the tabular data
    Preprocessing the columns
    Sorting lists
    Sorting with a key
    Working with itertools
2: PYTHON ENVIRONMENTS
    Introduction
    Using NumPy libraries
    Plotting with matplotlib
    Machine learning with scikit-learn
3: DATA ANALYSIS – EXPLORE AND WRANGLE
    Introduction
    Analyzing univariate data graphically
    Grouping the data and using dot plots
    Using scatter plots for multivariate data
    Using heat maps
    Performing summary statistics and plots
    Using a box-and-whisker plot
    Imputing the data
    Performing random sampling
    Scaling the data
    Standardizing the data
    Performing tokenization
    Removing stop words
    Stemming the words
    Performing word lemmatization
    Representing the text as a bag of words
    Calculating term frequencies and inverse document frequencies
4: DATA ANALYSIS – DEEP DIVE
    Introduction
    Extracting the principal components
    Using Kernel PCA
    Extracting features using singular value decomposition
    Reducing the data dimension with random projection
    Decomposing the feature matrices using non-negative matrix factorization
5: DATA MINING – NEEDLE IN A HAYSTACK
    Introduction
    Working with distance measures
    Learning and using kernel methods
    Clustering data using the k-means method
    Learning vector quantization
    Finding outliers in univariate data
    Discovering outliers using the local outlier factor method
6: MACHINE LEARNING 1
    Introduction
    Preparing data for model building
    Finding the nearest neighbors
    Classifying documents using Naïve Bayes
    Building decision trees to solve multiclass problems
7: MACHINE LEARNING 2
    Introduction
    Predicting real-valued numbers using regression
    Learning regression with L2 shrinkage – ridge
    Learning regression with L1 shrinkage – LASSO
    Using cross-validation iterators with L1 and L2 shrinkage
8: ENSEMBLE METHODS
    Introduction
    Understanding Ensemble – Bagging Method
    Understanding Ensemble – Boosting Method
    Understanding Ensemble – Gradient Boosting
9: GROWING TREES
    Introduction
    Going from trees to Forest – Random Forest
    Growing Extremely Randomized Trees
    Growing Rotational Forest
10: LARGE-SCALE MACHINE LEARNING – ONLINE LEARNING
    Introduction
    Using perceptron as an online learning algorithm
    Using stochastic gradient descent for regression
    Using stochastic gradient descent for classification